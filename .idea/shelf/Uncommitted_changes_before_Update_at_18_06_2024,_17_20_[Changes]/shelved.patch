Index: NLPFirst/main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pandas as pd\nimport time\nimport nltk\nimport spacy\nfrom nltk import PorterStemmer\nfrom collections import Counter\nimport re\nimport string\n\n\ndef print_word_statistics(words2, title):\n    word_counts2 = Counter(words2)\n    total_words = len(words2)\n    unique_words2 = len(word_counts2)\n    most_common_words2 = word_counts2.most_common(5)\n\n    print(f\"{title} Statistics:\")\n    print(f\"Total words: {total_words}\")\n    print(f\"Unique words: {unique_words2}\")\n    print(f\"Most common words: {most_common_words2}\")\n    print(\"\\n\")\n\n\ndef remove_punctuation(input_string):\n    # Define the translation table for removing punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    return input_string.translate(translator)\n\n\ndata = pd.read_csv('spam.csv', encoding=\"latin1\")\ndata = data.iloc[:, :-3]\nnlp = spacy.load('en_core_web_sm')\nprint(data.head())\nprint(f\"We have {len(data)} messages.\")\n# Extract the first column\nlabel_counts = data.iloc[:, 0].value_counts()\nprint(f\"We have {label_counts.get('ham', 0)} Ham and {label_counts.get('spam', 0)} Spam.\")\n# Extract the messages column\nmessages = data.iloc[:, 1]\nword_counts = messages.apply(lambda x: len(x.split()))  ## gets the number of words\naverage_word_count = word_counts.mean()  ## gets the mean of the numbers of words\nprint(f\"We have {average_word_count} words in average.\")\n\n# Convert all messages to a single string\nall_words = ' '.join(messages)\n# Remove any non-alphabetic characters and split into words\nwords = re.findall(r'\\b\\w+\\b', all_words.lower())\n# Count the occurrences of each word\n\nword_counts = Counter(words)\n# Get the 5 most common words\nmost_common_words = word_counts.most_common(5)\n# Get the words that appear only once\nwords_appear_once = [word for word, count in word_counts.items() if count == 1]\n# Print the 5 most frequent words\nprint(\"The 5 most frequent words are:\")\nfor word, count in most_common_words:\n    print(f\"{word}: {count}\")\nprint(f\"Words that appear only once:{len(words_appear_once)}\")\nstopwords = nltk.corpus.stopwords.words('english')\nfiltered_words = [word for word in words if word not in stopwords and word not in string.punctuation]\nfiltered_string = ' '.join(filtered_words)\n# NLTK TOKEN\nstart_time = time.time()\ntokens_nltk = nltk.word_tokenize(filtered_string)# need to stop duplicate and stop words, think where to do each step\n#stopwords = nltk.corpus.stopwords.words('english')\n#filtered_tokens = [token.lower() for token in tokens_nltk if token.lower() not in stopwords and token.isalpha()]\nend_time = time.time()\nprint(f\"It took {end_time - start_time:.2f} seconds to Tokenize with nltk\")\nprint_word_statistics(tokens_nltk,'NLTK After Tokenize')\nfiltered_tokens = list(set(tokens_nltk))# no Dups\n\n\n## SPACY TOKEN\nstart_time = time.time()\ntokens_spacy = nlp(filtered_string)\nend_time = time.time()\nprint(f\"It took {end_time - start_time:.2f} seconds to Tokenize with spacy\")\n#tokens_without_stopwords = [token.text for token in tokens_spacy if not token.is_stop and token.text not in string.punctuation]\ntokens_without_stopwords = list(set(tokens_spacy))\n#print_word_statistics(tokens_spacy,'Spacy After Token')\n\n# Create a set to store unique words\nunique_words = set()\n# Iterate through tokens\nfor token in tokens_spacy:\n    # Filter out stopwords, punctuation, and spaces\n    if not token.is_stop and not token.is_punct and not token.is_space:\n        # Add the token's lowercase form to the set\n        unique_words.add(token.text.lower())\n\n# Get the number of unique words\nfiltered_tokens = [token.text.lower() for token in tokens_spacy]\n\n# Count the frequency of each word\nword_freq = Counter(filtered_tokens)\n\n# Get the 5 most common words\nmost_common_words = word_freq.most_common(5)\nnum_unique_words = len(unique_words)\nprint(f'Number of Tokens :{len(tokens_spacy)}')\nprint(f\"Number of unique words: {num_unique_words}\")\nprint(f'Most 5 :{most_common_words}')\n\n#lem With nltk\nstart_time = time.time()\nlemmatizer = nltk.WordNetLemmatizer()\nlemmatizer_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\nend_time = time.time()\nprint(f\"It took {end_time - start_time:.2f} seconds to Lemmatize in nltk\")\nprint_word_statistics(lemmatizer_words,'Nltk After Lemmatization')\n\n\nstart_time = time.time()\nlemmatizer_words_spacy = [token.lemma_ for token in tokens_spacy]\nend_time = time.time()\nprint(f\"It took {end_time - start_time:.2f} seconds to lemmatize with spaCy\")\nprint_word_statistics(lemmatizer_words_spacy,'Spacy After Lem')\n\n\n# Stem in nltk\nstart_time = time.time()\nstemmer = PorterStemmer()\nstemmed_words = [stemmer.stem(word) for word in filtered_tokens]\nend_time = time.time()\nprint(f\"It took {end_time - start_time:.2f} seconds to Stem in nltk\")\nprint_word_statistics(stemmed_words,'NLTK in stem')\n    ## END OF TEXT PROCESSING
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/NLPFirst/main.py b/NLPFirst/main.py
--- a/NLPFirst/main.py	(revision 64ef2eae553a9e7e7535748d07ec7081e721b10f)
+++ b/NLPFirst/main.py	(date 1717682914646)
@@ -125,4 +125,4 @@
 end_time = time.time()
 print(f"It took {end_time - start_time:.2f} seconds to Stem in nltk")
 print_word_statistics(stemmed_words,'NLTK in stem')
-    ## END OF TEXT PROCESSING
\ No newline at end of file
+    ## END OF TEXT PROCESSING
