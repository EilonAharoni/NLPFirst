Index: NLPFirst/main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pandas as pd\nimport time\nimport nltk\nimport spacy\nfrom nltk import PorterStemmer\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\nfrom collections import Counter\nimport re\nfrom bs4 import BeautifulSoup\nimport requests\nimport string\n\n\ndef remove_punctuation(input_string):\n    # Define the translation table for removing punctuation\n    translator = str.maketrans('', '', string.punctuation)\n    return input_string.translate(translator)\n\n\n\n\n\ndata = pd.read_csv('spam.csv', encoding=\"latin1\")\n\ndata = data.iloc[:, :-3]\n#nlp = spacy.load('en_core_web_sm')\nprint(data.head())\nprint(f\"We have {len(data)} messages.\")\n# Extract the first column\nlabel_counts = data.iloc[:, 0].value_counts()\nprint(f\"We have {label_counts.get('ham', 0)} Ham and {label_counts.get('spam', 0)} Spam.\")\n# Extract the messages column\nmessages = data.iloc[:, 1]\nword_counts = messages.apply(lambda x: len(x.split()))  ## gets the number of words\naverage_word_count = word_counts.mean()  ## gets the mean of the numbers of words\nprint(f\"We have {average_word_count} words in average.\")\n\n# Convert all messages to a single string\nall_words = ' '.join(messages)\n# Remove any non-alphabetic characters and split into words\nwords = re.findall(r'\\b\\w+\\b', all_words.lower())\n# Count the occurrences of each word\n\nword_counts = Counter(words)\n# Get the 5 most common words\nmost_common_words = word_counts.most_common(5)\n# Get the words that appear only once\nwords_appear_once = [word for word, count in word_counts.items() if count == 1]\n# Print the 5 most frequent words\nprint(\"The 5 most frequent words are:\")\nfor word, count in most_common_words:\n    print(f\"{word}: {count}\")\nprint(f\"Words that appear only once:{len(words_appear_once)}\")\n\nstart_time = time.time()\ntokens_nltk = nltk.word_tokenize(all_words) # need to stop duplicate and stop words, think where to do each step\nstopwords = nltk.corpus.stopwords.words('english')\nfiltered_tokens = [token.lower() for token in tokens_nltk if token.lower() not in stopwords and token.isalpha()]\nend_time = time.time()\nprint(f\"It took {end_time - start_time:.2f} seconds to Tokenize with nltk\")\ntoken_counts = Counter(tokens_nltk)\nmost_common_words = token_counts.most_common(5)\nprint(f\"There Are {len(tokens_nltk)} Words after tokenization\")\nprint(\"The 5 most frequent words after tokenization in nltk are\")\nfor word, count in most_common_words:\n    print(f\"{word}: {count}\")\nfiltered_tokens = list(set(filtered_tokens)) # no Dups\n\n\n# start_time = time.time()\n# tokens_spacy = [token.text for token in nlp(all_words)]\n# end_time = time.time()\n# print(f\"It took {end_time - start_time:.2f} seconds to Tokenize with spacy\")\n\n\n#lem With nltk\nstart_time = time.time()\nlemmatizer = nltk.WordNetLemmatizer()\nlemmatizer_words = [lemmatizer.lemmatize(word) for word in filtered_tokens]\nend_time = time.time()\nprint(f\"It took {end_time - start_time:.2f} seconds to Lemmatize in nltk\")\nlemmatizer_count = Counter(lemmatizer_words)\nmost_common_words = lemmatizer_count.most_common(5)\nprint(f\"There Are {len(lemmatizer_words)} Words after Lemmatize\")\nprint(\"The 5 most frequent words after Lemmatize in nltk are\")\nfor word, count in most_common_words:\n    print(f\"{word}: {count}\")\n\n# # Lemmatization with spaCy\n# start_time = time.time()\n# # Create a spaCy document\n# doc_spacy = nlp(all_words)\n# # Extract lemmas\n# lemmatizer_words_spacy = [token.lemma_ for token in doc_spacy]\n# end_time = time.time()\n# print(f\"It took {end_time - start_time:.2f} seconds to lemmatize with spaCy\")\n\n\n\n# Stem in nltk\nstart_time = time.time()\nstemmer = PorterStemmer()\nstemmed_words = [stemmer.stem(word) for word in filtered_tokens]\nend_time = time.time()\nprint(f\"It took {end_time - start_time:.2f} seconds to Stem in nltk\")\nstem_count = Counter(stemmed_words)\nmost_common_words = stem_count.most_common(5)\nprint(f\"There Are {len(lemmatizer_words)} Words after Stem\")\nprint(\"The 5 most frequent words after Stem in nltk are\")\nfor word, count in most_common_words:\n    print(f\"{word}: {count}\")\n\n    ## END OF TEXT PROCESSING
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/NLPFirst/main.py b/NLPFirst/main.py
--- a/NLPFirst/main.py	
+++ b/NLPFirst/main.py	
@@ -59,7 +59,7 @@
 filtered_tokens = [token.lower() for token in tokens_nltk if token.lower() not in stopwords and token.isalpha()]
 end_time = time.time()
 print(f"It took {end_time - start_time:.2f} seconds to Tokenize with nltk")
-token_counts = Counter(tokens_nltk)
+token_counts = Counter(filtered_tokens)
 most_common_words = token_counts.most_common(5)
 print(f"There Are {len(tokens_nltk)} Words after tokenization")
 print("The 5 most frequent words after tokenization in nltk are")
